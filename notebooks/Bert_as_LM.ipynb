{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate sentence probability with BERT\n",
    "## From https://github.com/huggingface/transformers/issues/37, but updated to newest transformers version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efce5657bfb450ab6031e6a9a76cf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=314.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2828456463146d9b94ae20c820bd1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0892b3ffbc4693852d504f6ad04704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "    model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "    model.eval()\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_predictions(probs, index, k=5):\n",
    "    probs = probs.detach().numpy()\n",
    "    top_indexes = np.argpartition(probs, -k)[-k:]\n",
    "    sorted_indexes = top_indexes[np.argsort(-probs[top_indexes])]\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(sorted_indexes)\n",
    "    print(f\"Ordered top predicted tokens: {top_tokens}\")\n",
    "    print(f\"Ordered top predicted values: {probs[sorted_indexes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = '[CLS]'\n",
    "EOS_TOKEN = '[SEP]'\n",
    "MASK_TOKEN = '[MASK]'\n",
    "\n",
    "def get_sentence_prob(sentence):\n",
    "    sm = torch.nn.Softmax(dim=0) # used to convert last hidden state to probs\n",
    "    \n",
    "    # Pre-process sentence, adding special tokens\n",
    "    tokenized_input = tokenizer.tokenize(sentence)\n",
    "    if tokenized_input[0] != BOS_TOKEN:\n",
    "        tokenized_input.insert(0, BOS_TOKEN)\n",
    "    if tokenized_input[-1] != EOS_TOKEN:\n",
    "        tokenized_input.append(EOS_TOKEN)\n",
    "    ids_input = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "    print(f\"Processing sentence: {tokenized_input}\")\n",
    "    #print(f\"Sentence ids: {ids_input}\")\n",
    "    \n",
    "    sent_prob = 1\n",
    "    # Mask non-special tokens and calculate their probabilities\n",
    "    for i in range(1,len(tokenized_input)-1): # Ignore first and last tokens\n",
    "        current_tokenized = tokenized_input[:]\n",
    "        current_tokenized[i] = MASK_TOKEN\n",
    "        masked_input = torch.tensor([tokenizer.convert_tokens_to_ids(current_tokenized)])\n",
    "        outputs = model(masked_input)\n",
    "        predictions = outputs[0]\n",
    "        current_probs = sm(predictions[0, i]) # Softmax to get probabilities\n",
    "        current_prob = current_probs[ids_input[i]] # Prediction for masked word\n",
    "        sent_prob *= current_prob\n",
    "        \n",
    "        print(f\"Word: {tokenized_input[i]} \\t Prob: {current_prob}\")\n",
    "        #print_top_predictions(current_probs, ids_input[i])\n",
    "\n",
    "    print(f\"Sentence probability: {sent_prob.item()}\")\n",
    "    return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: ['[CLS]', 'he', 'was', 'born', 'in', 'berlin', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.7967859506607056\n",
      "Word: was \t Prob: 0.9999992847442627\n",
      "Word: born \t Prob: 0.9977497458457947\n",
      "Word: in \t Prob: 0.9979470372200012\n",
      "Word: berlin \t Prob: 0.02355594001710415\n",
      "Word: . \t Prob: 0.9999347925186157\n",
      "Sentence probability: 0.018687129020690918\n",
      "Processing sentence: ['[CLS]', 'he', 'was', 'born', 'in', 'santiago', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.7612152695655823\n",
      "Word: was \t Prob: 0.9999862909317017\n",
      "Word: born \t Prob: 0.9960402250289917\n",
      "Word: in \t Prob: 0.997549831867218\n",
      "Word: santiago \t Prob: 0.0008775214664638042\n",
      "Word: . \t Prob: 0.9998825788497925\n",
      "Sentence probability: 0.0006636204198002815\n",
      "Processing sentence: ['[CLS]', 'he', 'was', 'born', 'in', 'chile', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.6504054069519043\n",
      "Word: was \t Prob: 0.9999675750732422\n",
      "Word: born \t Prob: 0.9977337121963501\n",
      "Word: in \t Prob: 0.9998539686203003\n",
      "Word: chile \t Prob: 0.0006050239317119122\n",
      "Word: . \t Prob: 0.9999569654464722\n",
      "Sentence probability: 0.000392532063415274\n",
      "Processing sentence: ['[CLS]', 'he', 'was', 'born', 'in', 'window', '.', '[SEP]']\n",
      "Word: he \t Prob: 0.6543523073196411\n",
      "Word: was \t Prob: 0.9999873638153076\n",
      "Word: born \t Prob: 8.784436067799106e-05\n",
      "Word: in \t Prob: 0.7220567464828491\n",
      "Word: window \t Prob: 1.4590033288186532e-06\n",
      "Word: . \t Prob: 0.9915589094161987\n",
      "Sentence probability: 6.004352570698757e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.0044e-11, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_sentence_prob(\"I fed my cat some of it and he damn near passed out\")\n",
    "get_sentence_prob(\"He was born in Berlin.\")\n",
    "get_sentence_prob(\"He was born in Santiago.\")\n",
    "get_sentence_prob(\"He was born in Chile.\")\n",
    "get_sentence_prob(\"He was born in window.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
