{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate sentence probability with XLNet\n",
    "## Based and updated from https://github.com/huggingface/transformers/issues/37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93b4f883e79473d8a33fcebbcb9a722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=641.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfec8e903784b0ba00f0a1162a0bfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c66d3be09d54ac289dbe988d068b936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "    model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "    model.eval()\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_predictions(probs, index, k=5):\n",
    "    probs = probs.detach().numpy()\n",
    "    top_indexes = np.argpartition(probs, -k)[-k:]\n",
    "    sorted_indexes = top_indexes[np.argsort(-probs[top_indexes])]\n",
    "    top_tokens = tokenizer.convert_ids_to_tokens(sorted_indexes)\n",
    "    print(f\"Ordered top predicted tokens: {top_tokens}\")\n",
    "    print(f\"Ordered top predicted values: {probs[sorted_indexes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'Hello', ',', '▁my', '▁dog', '▁is', '▁very', '<mask>', '<sep>', '<cls>']\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is very <mask>\", add_special_tokens=True)).unsqueeze(0)  # We will predict the masked token\n",
    "tokenized_input = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TOKEN = '<mask>'\n",
    "\n",
    "def get_sentence_prob(sentence):\n",
    "    sm = torch.nn.Softmax(dim=0) # used to convert last hidden state to probs\n",
    "    \n",
    "    # Pre-process sentence, adding special tokens\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0)  # We will predict the masked token\n",
    "    tokenized_input = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    print(f\"Processing sentence: {tokenized_input}\")\n",
    "    print(f\"Processing sentence: {input_ids}\")\n",
    "    \n",
    "    sent_prob = 1\n",
    "    # Mask non-special tokens and calculate their probabilities\n",
    "    for i in range(0,len(tokenized_input)-2): # Ignore final tokens\n",
    "        current_tokenized = tokenized_input[:]\n",
    "        current_tokenized[i] = MASK_TOKEN\n",
    "        masked_input = torch.tensor([tokenizer.convert_tokens_to_ids(current_tokenized)])\n",
    "        \n",
    "        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "        perm_mask[:, :, i] = 1.0 \n",
    "\n",
    "        target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)\n",
    "        target_mapping[0, 0, i] = 1.0\n",
    "        \n",
    "        outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n",
    "        predictions = outputs[0]\n",
    "        print(predictions)\n",
    "        \n",
    "        current_probs = sm(predictions[0, -1]) # Softmax to get probabilities\n",
    "        current_prob = current_probs[input_ids[0][i]] # Prediction for masked word\n",
    "        sent_prob *= current_prob\n",
    "        \n",
    "        print(f\"Word: {tokenized_input[i]} \\t Prob: {current_prob}\")\n",
    "        #print_top_predictions(current_probs, ids_input[i])\n",
    "\n",
    "    print(f\"\\nSentence probability: {sent_prob.item()}\\n\")\n",
    "    return sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: ['▁He', '▁was', '▁born', '▁in', '▁Berlin', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  69,   30, 1094,   25, 4158,    9,    4,    3]])\n",
      "tensor([[[-30.8018, -42.2576, -41.8839,  ..., -37.6388, -36.3895, -40.6825]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁He \t Prob: 0.0003586793318390846\n",
      "tensor([[[-26.9867, -39.1739, -38.9740,  ..., -32.0969, -32.5971, -36.2555]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁was \t Prob: 0.006640327163040638\n",
      "tensor([[[-35.5227, -43.8349, -43.6165,  ..., -36.6232, -40.9318, -45.7496]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁born \t Prob: 9.768486052053049e-05\n",
      "tensor([[[-34.4562, -44.3209, -43.8899,  ..., -36.0626, -38.1389, -43.3396]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁in \t Prob: 0.17678529024124146\n",
      "tensor([[[-29.7587, -45.5898, -45.4703,  ..., -42.9988, -34.7171, -37.9252]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁Berlin \t Prob: 9.891665797567839e-08\n",
      "tensor([[[-31.5731, -44.9830, -44.7597,  ..., -36.9262, -35.1503, -39.9347]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: . \t Prob: 0.4354376196861267\n",
      "\n",
      "Sentence probability: 1.7715954653716646e-18\n",
      "\n",
      "Processing sentence: ['▁He', '▁was', '▁born', '▁in', '▁Santiago', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[   69,    30,  1094,    25, 13960,     9,     4,     3]])\n",
      "tensor([[[-29.8673, -40.9866, -40.6398,  ..., -37.5227, -34.7027, -40.5934]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁He \t Prob: 8.160410652635619e-05\n",
      "tensor([[[-28.7243, -38.7353, -38.4553,  ..., -35.5658, -33.2996, -38.8202]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁was \t Prob: 0.04169566556811333\n",
      "tensor([[[-36.3038, -44.1696, -44.0297,  ..., -37.4396, -40.0684, -46.3872]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁born \t Prob: 0.0002508793259039521\n",
      "tensor([[[-32.3747, -43.8918, -43.5302,  ..., -38.1687, -37.5864, -41.3584]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁in \t Prob: 0.1876431256532669\n",
      "tensor([[[-29.7587, -45.5898, -45.4703,  ..., -42.9988, -34.7171, -37.9252]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁Santiago \t Prob: 7.654647055099773e-11\n",
      "tensor([[[-31.4773, -43.8757, -43.7267,  ..., -37.1232, -35.5245, -39.9918]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: . \t Prob: 0.12329649180173874\n",
      "\n",
      "Sentence probability: 1.5117372939002096e-21\n",
      "\n",
      "Processing sentence: ['▁He', '▁was', '▁born', '▁in', '▁Chile', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  69,   30, 1094,   25, 7224,    9,    4,    3]])\n",
      "tensor([[[-30.3643, -40.8244, -40.6852,  ..., -37.4422, -33.0407, -40.9620]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁He \t Prob: 1.1714562788256444e-05\n",
      "tensor([[[-27.9534, -38.5761, -38.0846,  ..., -35.7389, -31.0092, -37.1915]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁was \t Prob: 0.05628563091158867\n",
      "tensor([[[-36.6540, -44.0265, -43.9331,  ..., -35.6300, -39.3648, -46.0872]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁born \t Prob: 0.00015316132339648902\n",
      "tensor([[[-36.0463, -45.8475, -45.4229,  ..., -40.0042, -39.1367, -43.7553]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁in \t Prob: 0.15276795625686646\n",
      "tensor([[[-29.7587, -45.5898, -45.4703,  ..., -42.9988, -34.7171, -37.9252]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁Chile \t Prob: 5.7609792492030465e-09\n",
      "tensor([[[-32.2383, -45.5227, -45.3108,  ..., -38.3756, -35.2189, -40.1703]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: . \t Prob: 0.3053362965583801\n",
      "\n",
      "Sentence probability: 2.7138122149594125e-20\n",
      "\n",
      "Processing sentence: ['▁He', '▁was', '▁born', '▁in', '▁window', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  69,   30, 1094,   25, 2078,    9,    4,    3]])\n",
      "tensor([[[-34.6991, -45.5315, -45.3714,  ..., -41.8901, -37.6147, -45.7502]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁He \t Prob: 1.1008211004082114e-05\n",
      "tensor([[[-26.7258, -39.2744, -38.7602,  ..., -32.4309, -33.8196, -38.9296]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁was \t Prob: 2.811039303196594e-05\n",
      "tensor([[[-34.6715, -42.8887, -42.5949,  ..., -38.2324, -37.1518, -42.9161]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁born \t Prob: 7.630734444319387e-07\n",
      "tensor([[[-33.6625, -45.6438, -45.2613,  ..., -40.9483, -38.9205, -44.9063]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁in \t Prob: 0.010454324074089527\n",
      "tensor([[[-29.7587, -45.5898, -45.4703,  ..., -42.9988, -34.7171, -37.9252]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: ▁window \t Prob: 4.4989826619712403e-07\n",
      "tensor([[[-26.9873, -40.1533, -39.7322,  ..., -35.2768, -27.9877, -34.7865]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Word: . \t Prob: 0.5752824544906616\n",
      "\n",
      "Sentence probability: 6.389125480271783e-25\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.3891e-25, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_sentence_prob(\"I fed my cat some of it and he damn near passed out\")\n",
    "get_sentence_prob(\"He was born in Berlin.\")\n",
    "get_sentence_prob(\"He was born in Santiago.\")\n",
    "get_sentence_prob(\"He was born in Chile.\")\n",
    "get_sentence_prob(\"He was born in window.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: ['▁I', '▁fed', '▁my', '▁cat', '▁some', '▁of', '▁it', '▁and', '▁he', '▁damn', '▁near', '▁passed', '▁out', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  35, 8124,   94, 4777,  106,   20,   36,   21,   43, 7757,  479, 1400,\n",
      "           78,    9,    4,    3]])\n",
      "Word: ▁fed \t Prob: 0.0006848873454146087\n",
      "Word: ▁my \t Prob: 0.0032348744571208954\n",
      "Word: ▁cat \t Prob: 7.678403198951855e-05\n",
      "Word: ▁some \t Prob: 0.0014662300236523151\n",
      "Word: ▁of \t Prob: 0.6216524243354797\n",
      "Word: ▁it \t Prob: 0.18512940406799316\n",
      "Word: ▁and \t Prob: 0.11187209188938141\n",
      "Word: ▁he \t Prob: 0.0001272847002837807\n",
      "Word: ▁damn \t Prob: 3.8606871385127306e-05\n",
      "Word: ▁near \t Prob: 0.0004729019710794091\n",
      "Word: ▁passed \t Prob: 0.006773877423256636\n",
      "Word: ▁out \t Prob: 0.001013346598483622\n",
      "Word: . \t Prob: 0.001979386666789651\n",
      "\n",
      "Sentence probability: 1.0139868223547045e-34\n",
      "\n",
      "Processing sentence: ['▁I', '▁fed', '▁my', '▁dog', '▁some', '▁of', '▁it', '▁and', '▁he', '▁damn', '▁near', '▁passed', '▁out', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  35, 8124,   94, 2288,  106,   20,   36,   21,   43, 7757,  479, 1400,\n",
      "           78,    9,    4,    3]])\n",
      "Word: ▁fed \t Prob: 0.00010342417226638645\n",
      "Word: ▁my \t Prob: 0.005683479830622673\n",
      "Word: ▁dog \t Prob: 0.00012695563782472163\n",
      "Word: ▁some \t Prob: 0.0031110765412449837\n",
      "Word: ▁of \t Prob: 0.6952401995658875\n",
      "Word: ▁it \t Prob: 0.1748540848493576\n",
      "Word: ▁and \t Prob: 0.07472626864910126\n",
      "Word: ▁he \t Prob: 0.002467647660523653\n",
      "Word: ▁damn \t Prob: 7.284965977305546e-05\n",
      "Word: ▁near \t Prob: 0.0005069577600806952\n",
      "Word: ▁passed \t Prob: 0.002208008198067546\n",
      "Word: ▁out \t Prob: 0.0021731178276240826\n",
      "Word: . \t Prob: 0.0012496161507442594\n",
      "\n",
      "Sentence probability: 1.1524604471824286e-33\n",
      "\n",
      "Processing sentence: ['▁I', '▁fed', '▁my', '▁window', '▁some', '▁of', '▁it', '▁and', '▁he', '▁damn', '▁near', '▁passed', '▁out', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  35, 8124,   94, 2078,  106,   20,   36,   21,   43, 7757,  479, 1400,\n",
      "           78,    9,    4,    3]])\n",
      "Word: ▁fed \t Prob: 9.837235666054767e-07\n",
      "Word: ▁my \t Prob: 0.005064073950052261\n",
      "Word: ▁window \t Prob: 1.5972525488905376e-06\n",
      "Word: ▁some \t Prob: 0.0004741363227367401\n",
      "Word: ▁of \t Prob: 0.18112626671791077\n",
      "Word: ▁it \t Prob: 0.1467614620923996\n",
      "Word: ▁and \t Prob: 0.07558763027191162\n",
      "Word: ▁he \t Prob: 0.00021011281933169812\n",
      "Word: ▁damn \t Prob: 0.0001561342360218987\n",
      "Word: ▁near \t Prob: 0.000595681369304657\n",
      "Word: ▁passed \t Prob: 0.005785761401057243\n",
      "Word: ▁out \t Prob: 0.00037108181277289987\n",
      "Word: . \t Prob: 7.951575389597565e-05\n",
      "\n",
      "Sentence probability: 2.5289233385669974e-41\n",
      "\n",
      "Processing sentence: ['▁I', '▁fed', '▁my', '▁the', '▁some', '▁of', '▁it', '▁and', '▁he', '▁damn', '▁near', '▁passed', '▁out', '.', '<sep>', '<cls>']\n",
      "Processing sentence: tensor([[  35, 8124,   94,   18,  106,   20,   36,   21,   43, 7757,  479, 1400,\n",
      "           78,    9,    4,    3]])\n",
      "Word: ▁fed \t Prob: 1.0282243238179944e-05\n",
      "Word: ▁my \t Prob: 0.00118632975500077\n",
      "Word: ▁the \t Prob: 0.0026144515722990036\n",
      "Word: ▁some \t Prob: 0.0013217190280556679\n",
      "Word: ▁of \t Prob: 0.20140135288238525\n",
      "Word: ▁it \t Prob: 0.023352261632680893\n",
      "Word: ▁and \t Prob: 0.059329986572265625\n",
      "Word: ▁he \t Prob: 0.00263643148355186\n",
      "Word: ▁damn \t Prob: 3.4455169952707365e-05\n",
      "Word: ▁near \t Prob: 0.00016930251149460673\n",
      "Word: ▁passed \t Prob: 0.0029024251271039248\n",
      "Word: ▁out \t Prob: 0.0009483885369263589\n",
      "Word: . \t Prob: 0.005403789225965738\n",
      "\n",
      "Sentence probability: 2.6906587858756214e-36\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.6907e-36, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_prob(\"I fed my cat some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my dog some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my window some of it and he damn near passed out.\")\n",
    "get_sentence_prob(\"I fed my the some of it and he damn near passed out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
